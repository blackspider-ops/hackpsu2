[
    {
        "title": "Overview",
        "url": "https://icds-docs.readthedocs.io/en/latest/01_Overview/",
        "content": "TheInstitute for Computational and Data Sciences (ICDS)is one of seven interdisciplinary research institutes within Penn State's Office of the Senior Vice President for Research. \nThe mission of ICDS is to build capacity to solve problems of scientific and societal importance through cyber-enabled research. \nICDS enables and supports the diverse computational and data science research taking place throughout Penn State.\nICDS provides university faculty, staff, students, and collaborators access to Roar, which consists of theRoar Collab (RC)andRoar Restricted (RR)research computing clusters. \nRoar Collab is the flagship computing cluster for Penn State researchers. \nAccess to Roar Restricted is provided on an as-needed basis to research groups specifically handling restricted data. \nMost of the material within this ICDS User Guide is common to both Roar Collab and Roar Restricted, but some sections may specifically refer to Roar Collab. \nTheRoar Restricted Addendumspecifically addresses items unique to Roar Restricted.\nA computing cluster is a group of interconnected computers that work together to perform computational tasks. \nEach of these computers, referred to as anode, typically consists of its own processor(s), memory, and network interface. \nOften these nodes are additionally connected to a shared filesystem so data is available to all of the nodes. \nNodes can be used individually but can also be utilized collectively to perform demanding computational processes more efficiently. \nIntegrating the separate nodes into a single system requires a suite of cluster management software that is configured and implemented by system administration personnel.\nThe research computing clusters offered by ICDS are shared computational resources, so the computational processes must be managed to allow for many simultaneous users. \nTo perform computationally intensive tasks, users must request compute resources and be provided access to those resources to perform the tasks. \nA computational process or workflow run via the request and provision of computational resources is typically referred to as a computationaljob.\nThe request and provision process allows the tasks of many users to be scheduled and carried out efficiently to avoid resource contention.\nThe software tasked with managing the scheduling of computational resources is often referred to as theresource manageror thejob scheduler.\nShared computing clusters are often configured to mediate access to its pool of computational resources not only through the use of a resource manager, but also by the cluster architecture.\nFrom an architectural perspective, in addition to the set of compute nodes that act as the pool of computational resources, a cluster often utilizes a set of nodes that specifically handle user logins and then also has other auxiliary nodes used for specific system administration functions.\nA user typically connects to a login-type node, and then requests computational resources via the resource management software. A node that is configured to handle user logins and the submission of resource requests is referred to as asubmit node, while a node used for computationally intensive tasks is referred to as acompute node.\nSlurm (Simple Linux Utility for Resource Management)is the software that acts as the job scheduler and resource manager.\nSlurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for Linux clusters. Its primary functions are to\nSlurm is rapidly rising in popularity and many other computing clusters use Slurm as well. TheSubmitting Jobssection provides further detail on the use of Slurm on Roar, andSlurm's documentationis a great resource for in-depth details on the usage of Slurm.\nThe ICDS research computing clusters are heterogeneous and somewhat dynamic. To see the different node configurations available, use the following command:\nThissinfocommand displays not only the core and memory configuration of the nodes, but it also indicates the processor generation associated with each node. Furthermore, while connected to a specific node, thelscpucommand provides more detailed information on the specific processor type available on the node. The first column of the output lists the features associated with each block of nodes\nTo add a column to thesinfocommand output that indicates the number of GPU(s) associated with each of the node blocks, simply add thegresoption to thesinfoformat string:\nOn a GPU node, running thenvidia-smicommand displays more detailed information on the GPU(s) available on that node.\nSlurm'ssinfodocumentation page provides a detailed description of the function and options of thesinfocommand.\nRoar is comprised of two distinct research computing clusters. \nTheRoar Collab (RC)andRoar Restricted (RR)system specifications are described below.\nRoar Collab (RC) is the flagship computing cluster for Penn State researchers. \nDesigned with collaboration in mind, the RC environment allows for more frequent software updates and hardware upgrades to keep pace with researchers\u2019 changing needs. \nRC utilizes the Red Hat Enterprise Linux (RHEL) 8 operating system to provide users with access to compute resources, file storage, and software. RC is a heterogeneous computing cluster comprised of different types of compute nodes, each of which can be categorized as a Basic, Standard, High-Memory, GPU, or Interactive node.\n\nRoar Restricted (RR) is designed for research groups that handle restricted data. \nRR is only accessible when connecting either via the Penn State network or via Penn State GlobalProtect VPN. \nAdditionally, transferring data to/from RR performed using RR's Secure Data Transfer Management Model.\nRR utilizes the Red Hat Enterprise Linux (RHEL) 8 operating system to provide users with access to compute resources, file storage, and software. RR is a heterogeneous computing cluster comprised of different types of compute nodes, each of which can be categorized as a Standard, GPU, or Interactive node.\n\nAdhering to the recommended best practices on Roar ultimately improves system functionality and stability.\nRoar is shared by many users, and a user's operating behavior can inadvertently impact system functionality for other users. \nExercise good citizenship to mitigate the risk of adversely impacting the system and the ICDS research community.\nDon't!Do Not Perform Computationally Intensive Tasks On Submit Nodes\nThe submit nodes (with asubmit*hostname) are not configured to handle intensive computational tasks. Dozens, and sometimes hundreds, of users may be logged on at any one time. Think of the submit nodes as a prep area, where users may edit and manage files, initiate file transfers, submit new jobs, and track existing jobs. The submit nodes serve as an interface to the system and to the computational resources.\nPerform intensive computations, like utilizing research software and conducting development and debugging sessions, on compute nodes. To access compute nodes, either submit a batch job or request an interactive session. TheSubmitting Jobssection provides further details on requesting computational resources.\nSince the submit nodes are not configured for intensive computations, the computational performance is poor. Additionally, running computationally intensive or disk intensive tasks on a submit node negatively impacts performance for other users. Habitually running jobs on the submit nodes can potentially lead to account suspension.\nDo!Remain Cognizant of Storage Quotas\nAll available storage locations have associated quotas. If the usage of a storage location approaches these quotas, software may not function nominally and may produce cryptic error messages. TheHandling Datasection provides further details on checking storage usage relative to the quotas.\nDon't!Do Not Use Scratch as a Primary Storage Location\nScratch serves as a temporary repository for compute output and is explicitly designed for short-term usage. Unlike other storage locations, scratch is not backed up. Files are subject to automatic removal after 30 days. Only utilize scratch for files that are non-critical and/or can be easily regenerated. TheHandling Datasection provides further details on storage options.\nDo!Try To Minimize Resource Requests\nThe amount of time jobs are queued grows as the amount of requested resources increases. To minimize the amount of time a job is queued, minimize the amount of resources requested. It is best to run small test cases to verify that the computational workflow runs successfully before scaling up the process to a large dataset. TheSubmitting Jobssection provides further details on requesting computational resources.\nThe policies regarding the use of Roar can be found on theICDS Policiespage."
    },
    {
        "title": "Connecting",
        "url": "https://icds-docs.readthedocs.io/en/latest/02_Connecting/",
        "content": "All individuals with an active Penn State access account and a Penn State email address may request access to Roar by submitting anaccount request. \nA Principal Investigator (PI) must be specified to request an account. \nThe PI must be Penn State faculty and should be a supervisor, advisor or collaborator.\nRoar Collab (RC) accounts are granted to any users upon PI approval, but Roar Restricted (RR) accounts are only granted to individuals that require access to an active restricted storage allocation. For additional information on RR account activation, see theRoar Restricted Addendum.\nFor any external collaborators, a university faculty member must set up asponsored access accountwith the university Accounts Office to provide the collaborator with an access account and a Penn State email address. \nOnce the collaborator's access account is active, submit anaccount request.\nUsers can connect to RC either through theRC Portal(rcportal.hpc.psu.edu) or via ansshconnection to thesubmit.hpc.psu.eduhost.\nUsers can only connect to RR via theRR Portal(rrportal.hpc.psu.edu), and RR is only accessible when connecting either via the Penn State network or via the Penn State GlobalProtect VPN. \nFor additional information on connecting to RR, see theRoar Restricted Addendum.\nUsers can connect to Roar through the Roar Portals powered by Open OnDemand. \nOpen OnDemand is an NSF-funded, open-source HPC portal that provides users with a simple graphical web interface to HPC resources. \nUsers can submit and monitor jobs, manage files, and run applications using just a web browser.\nThe Roar Portals feature multiple built-in tools which can be accessed via the top menu bar on the Portals:\nFile Manager tool is disabled on RR Portal.\nIn accordance with RR's Secure Data Transfer Management Model, the File Manager tool on the RR Portal is disabled. For additional information on the file transfer process for RR, see theRoar Restricted Addendum.\nThose who prefer to utilize only the command line environment can connect to RC using Secure Shell (SSH). \nSSH access for RR is disabled in accordance with security requirements for handling restricted data.\nThrough the terminal on macOS or Linux or the command prompt on Windows, users can connect to RC using the following command:\nTo connect, an RC account linked to an active Penn State access account user ID and password is required. \nBy default, port 22 is used for secure shell connections. \nA password must be entered and then multi-factor authentication must be completed successfully to complete the login.\nDo Not Perform Computationally Intensive Tasks On Submit Nodes\nThe connection to the system is made with a submit node. Submit nodes are configured primarily to handle incoming user connections and non-intensive computational tasks like editing small files. To perform computational tasks, compute resources must be used. SeeSubmitting Jobsfor more details.\nSpecial characters are useful in many commands.\nFor complete details on any command listed above and more, useman <command>in a terminal session to display the manual page for the command or search online for more detailed usage of fundamental Linux commands."
    },
    {
        "title": "Submitting Jobs",
        "url": "https://icds-docs.readthedocs.io/en/latest/03_SubmittingJobs/",
        "content": "The Roar computing clusters are shared computational resources. \nTo perform computationally intensive tasks, users must request compute resources and be provided access to those resources. \nThe request/provision process allows the tasks of many users to be scheduled and carried out efficiently to avoid resource contention.Slurmis utilized by Roar as the job scheduler and resource manager. \nSlurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for Linux clusters. \nSlurm is rapidly rising in popularity and many other HPC systems use Slurm as well. \nIts primary functions are to\nWarning\nDo not perform computationally intensive tasks on submit nodes. Submit a resource request via Slurm for computational resources so your computational task can be performed on a compute node.\nA compute session can be reached via either a batch job or an interactive job. \nThe following sections provide more details on intiating compute sessions:Interactive Jobs,Interactive Jobs Through the Roar Portal, andBatch Jobs.\nResource directives are used to request a specific set of compute resources for a compute session. \nThe following table lists some of the most useful resource directives.\nBoth standard output and standard error are directed to the same file by default, and the file name isslurm-%j.out, where the%jis replaced by the job ID. \nThe output and error filenames are customizable, however, using the table of symbols below.\nSlurm makes use of environment variables within the scope of a job, and utilizing these variables can be beneficial in many cases.\nFurther details on the available resource directives for Slurm are defined by Slurm in the documentation of thesallocandsbatchcommands.\nThe resource directives should be populated with resource requests that are adequate to complete the job but should be minimal enough that the job can be placed somewhat quickly by the scheduler. \nThe total time to completion of a job is the sum of the time the job is queued plus the time it takes the job to run to completion once placed. \nThe queue time is minimized when the bare minimum amount of resources are requested, and the queue time grows as the amount of requested resources grows. \nThe run time of the job is minimized when all the computational resources available to the job are efficiently utilized. \nThe total time to completion, therefore, is minimized when the resources requested closely match the amount of computational resources that can be efficiently utilized by the job. \nDuring the development of the computational job, it is best to keep track of an estimate of the computational resources used by the job. \nAdd about a 20% margin on top of the best estimate of the job's resource usage to produce practical resource requests used in the scheduler directives.\nIt's useful to examine the amount of resources that a single laptop computer has, or1 laptop-worth of resources, as a reference. \nA modern above-average laptop, for example, may have an 8-core processor and 32 GB of RAM. \nIf a computational task can run on a laptop without crashing the device, then there is absolutely no need to submit a resource request larger than this unless the job can efficiently utilize the additional resources.\nThe submit nodes are designed to handle very simple tasks such as connections, file editing, and submitting jobs. \nPerforming intensive computations on submit nodes will not only be computationally inefficient, but it will also adversely impact other users' ability to interact with the system. \nFor this reason, users that want to perform computations interactively should do so on compute nodes within an interactive compute session. \nSlurm'ssalloccommand is used to request an interactive compute session. \nTo work interactively on a compute node with a single processor core for one hour, use the following command:\nEquivalently, short options can be used instead of the long options, so the abbreviated version of this command is instead\nThe above commands submit a request to the scheduler to queue an interactive job, and when the scheduler is able to place the request, the prompt will return. \nThe hostname in the prompt will change from the previous submit node name to a compute node. \nNow on a compute node, intensive computational tasks can be performed interactively. \nThis session is terminated either when the time limit is reached or when theexitcommand is entered. \nAfter the interactive session completes, the session will return to the previous submit node.\nThe Roar Portals are simple graphical web interfaces that provide users with access to Roar. \nUsers can submit and monitor jobs, manage files, and run applications using just a web browser. \nTo access the Roar Portals, users must log in using valid Penn State access account credentials and must also have an account on Roar.\nTheRC Portalis available at the following webpage:https://rcportal.hpc.psu.edu\nTheRR Portalis available at the following webpage:https://rrportal.hpc.psu.edu\nUsers can run batch jobs by submitting scripts to the Slurm job scheduler. \nA Slurm script must do three things:\nThe portion of the job that prescribes the resource requirements contains the resource directives. \nResource directives in Slurm submission scripts are denoted by lines starting with the#SBATCHkeyword. \nThe rest of the script, which both sets the environment and specifies the work to be done, consists of shell commands. \nThe very first line of the submission script,#!/bin/bash, is called ashebangand specifies that the commands in the script are to be interpreted by the bash shell in this case.\nBelow is a sample Slurm script for running a Python script:\nIn this sample submission script, the resource directives request a single node with a singletask. \nSlurm is a task-based scheduler, and a task is equivalent to a processor core unless otherwise specified in the submission script. \nThe scheduler directives then request 1 GB of memory per node for a maximum of 1 minute of runtime. \nThe memory can be specified in KB, MB, GB, or TB by using a suffix of K, M, G, or T, respectively. \nIf no suffix is used, the default is MB. \nLastly, the work to be done is specified, which is the execution of a Python script in this case.\nIf the above sample submission script was saved aspyjob.slurm, it would be submitted to the Slurm scheduler with the followingsbatchcommand.\nThe job can be submitted to the scheduler from any node. \nIt is important to note, however, that any jobs submitted from within another computational session will inherit any Slurm environment variables that are not reset within the submitted job.\nThe scheduler will keep the job in the job queue until the job gains sufficient priority to run on a compute node. \nDepending on the nature of the job and the availability of computational resources, the queue time can vary between seconds to days. \nTo check the status of queued and running jobs, use thesqueuecommand:\nAll RC users have access to theopencompute account, which allows users to submit jobs free of charge. \nRR does not offer a free compute account, so users must submit jobs to a compute account provided by a paid compute allocation.\nUnder theopencompute account on RC, theopen,short, andicpartitions are available.\nAny resource limitations associated with compute partitions are set by Slurm via a Quality of Service (QOS) with the same name as the partition.\nThe per-user resource limits defined by a QOS for any partition can be displayed with the following command:\nFor example, the per-user limits for theopenpartition are displayed by the following command:\nJobs on theopencompute account will start and run only when sufficient idle compute resources are available. \nFor this reason, there is no guarantee on when anopenjob will start. \nAll users have equal priority on theopencompute account, butopenjobs have a lower priority than jobs submitted to a paid compute account. \nIf compute resources are required for higher priority jobs, then anopenjob may be cancelled so that the higher priority job can be placed. \nThe cancellation of a running job to free resources for a higher priority job is called preemption. \nBy using the--requeueoption in a submission script, a job will re-enter the job queue automatically if it is preempted. \nFurthermore, it is highly recommended for users to break down any large computational workflows into smaller, more manageable computational units so jobs can save state throughout the stages of the workflow. \nSaving state at set checkpoints will allow the computational workflow to return to the latest checkpoint, reducing the amount of required re-computation in the case that a job is interrupted for any reason. \nRC has somewhat low utilization, however, so the vast majority ofopenjobs can be submitted and placed in a reasonable amount of time. \nTheopencompute account is entirely adequate for most individual users and for many use cases.\nTheopencompute account can be specified using the--account openor-A openresource directive. To specify the partition, the--partition <partition>or-p <partition>resource directive is used.\nA paid compute allocation provides access to specific compute resources for an individual user or for a group of users. \nA paid compute allocation provides the following benefits:\nA compute allocation results in the creation of a compute account on Roar. \nThemybalancecommand on Roar lists accessible compute accounts and resource information associated with those compute accounts. \nUse themybalance -hoption for additional command usage information.\nTo submit a job to a paid compute account, supply the--accountor-Aresource directive with the compute account name and supply the--partitionor-presource directive withsla-prio:\nTo enable bursting, if enabled for the compute account, supply the--partition burstor-p burstresource directive.\nFurthermore, the desired level of bursting for the job (burst2x,burst3x,burst4x, and so on) must be specified using the--qosresource directive. \nTo list the available compute accounts and the associated available burst partitions, use the following command:\nThe principal contact for a compute allocation is automatically designated as a coordinator for the compute account associated with the compute allocation. \nA coordinator can add or remove another coordinator with the following command:\nA coordinator can then add and remove users from the compute account using the following:\nA paid compute allocation will typically cover a certain number of cores across a certain timeframe. \nThe resources associated with a compute allocation are in units ofcore-hours. \nThe compute allocation has an associatedLimitincore-hoursbased on the initial compute allocation agreement. \nAny amount of compute resources used on the compute allocation results in an accrual of the compute allocation'sUsage, again incore-hours. \nThe compute allocation'sBalanceis simply theLimitminus itsUsage.\nAt the start of the compute allocation, 60 days-worth of compute resources are added to the compute allocation'sLimit. \nEach day thereafter, 1 day-worth of compute resources are added to theLimit.\nThe daily replenishment scheme continues on schedule for the life of the compute allocation. \nNear the very end of the compute allocation, the replenishment schedule may be impacted by the enforced limit on the maximum allowableBalance. \nTheBalancefor a compute allocation cannot exceed the amount of compute resources for a window of 91 days and cannot exceed the amount usable by a 4x burst for the remaining life of the compute allocation. \nThis limit is only relevant for the replenishment schedule nearing the very end of the compute allocation life.\nGPUs are available to users that are added to paid GPU compute accounts. \nTo request GPU resources, use the--gpusresource directive:\nRequesting GPU resources for a job is only beneficial if the software running within the job is GPU-enabled.\nA user can find the job ID, the assigned node(s), and other useful information using thesqueuecommand. \nSpecifically, the following command displays all running and queued jobs for a specific user:\nA useful environment variable is theSQUEUE_FORMATvariable which enables customization of the details shown by thesqueuecommand. \nThis variable can be set, for example, with the following command to provide a highly descriptivesqueueoutput:\nFurther details on the usage of this variable are available on Slurm'ssqueuedocumentation page.\nAnother useful job monitoring command is:\nAlso, a job can be cancelled with\nValuable information can be obtained by monitoring a job on the compute node(s) as the job runs. \nConnect to the compute node of a running job with thesshcommand. \nNote that a compute node can only be reached if the user has a resource reservation on that specific node. \nAfter connecting to the compute node, thetopandpscommands are useful tools.\nSlurm's commands and scheduler directives can be mapped to and from PBS/Torque commands and scheduler directives. \nTo convert any PBS/Torque scripts and/or workflows to Slurm, the commands and scheduler directives should be swapped out and reconfigured. \nSee the table below for the mapping of some common commands and scheduler directives:\nFor a more complete list of command, scheduler directive, and option comparisons, see theSlurm Rosetta Stone."
    },
    {
        "title": "Handling Data",
        "url": "https://icds-docs.readthedocs.io/en/latest/04_HandlingData/",
        "content": "Roar Collab (RC) offers many file transfer options, and these methods are described in the following sections on this page.\nTo comply with restricted data storage standards, Roar Restricted (RR) must adhere to a Secure Data Transfer Management Model. \nThe data transfer process is described in detail in theRoar Restricted Addendum.\nRoar offers several file storage options for users, each with their own quotas and data retention policies. \nThe multiple options are available for users to optimize their workflows.\nHome should primarily be used for configuration files and should not be used as a primary storage location for data. \nWork should be used as the primary personal data storage location. \nScratch should be used for temporary files and for reading and writing large data files.\nTo provide a user with access to a paid group storage allocation, the owner of the storage allocation should submit a request toicds@psu.eduto add the user to their<owner>_collabgroup.\nHome should primarily be used for configuration files and should not be used as a primary storage location for data. \nWork should be used as the primary personal data storage location. \nThere is no scratch filesystem location available on RR.\nTo provide a user with access to a restricted storage allocation, the owner of the restricted storage allocation should submit a request toicds@psu.eduto add the user to their<owner>_collabgroup.\nTo check storage usage against the storage quotas, run the following command on Roar:\nThe outputs generated by these scripts are not necessarily generated in real-time, but the underlying quota information is updated several times per day. \nAfter removing many files, for instance, the updates to the storage usage may not be reflected in the outputs until the next update period.\nFor a real-time look into the memory usage for a particular storage location, navigate to the storage location and run the following command:\nFor a real-time look into the number of files in a storage location, navigate to the storage location and run the following command:\nA user can check the storage usage of an accessible group storage location by running the following command:\nHome is the primary location for configuration files, and many software packages will automatically place configuration files in this location. \nSometimes, these configuration files can grow in size such that the Home directory approaches its storage quota limit. \nIf this issue occurs, it is simple to move the configuration files from Home to Work and place a link in \nHome that points to the new location of the configuration files in Work.\nFor instance, Anaconda stores its configuration files in~/.condaby default, and this directory often grows to multiple GBs in size, consequently using a significant portion of the Home directory's allocated memory. \nThe~/.condadirectory can be moved to Work and can be replaced by a link in Home that points to the new location. \nThis can be carried out with the following commands:\nA paid storage allocation provides access to a shareable group location for active file storage. \nActive group storage is included with a paid compute allocation, but additional storage space can be purchased separately as well. \nActive storage is mounted on all compute resources and enables users to read, write, and modify files stored there. \nFor long-term storage of infrequently used files that is separate from compute resources, archive storage is available for purchase and is accessible via theGlobusinterface.\nTypically, access to group storage locations can be managed using the<owner>_collabgroup where the<owner>is the user ID of the owner of the group space. \nFor users to be added or removed from<owner>_collabgroups, the owner of that group must submit a request toicds@psu.edu.\nIf the owner of a group space would like more control over the access groups or would like to designate a group coordinator, then it is recommended that the ownercreate a User Managed Group (UMG). \nThe UMG allows a user to manage the group access list and group roles directly through the User Managed Group functionality throughPenn State Accounts Management. \nSelect the following options and adhere to the following recommendations when creating the UMG:\nICDS filters UMGs for display names that begin withicds.rc., so any UMGs created with this prefix will automatically appear within RC. \nIt may take up to 15 minutes for a newly created UMG to appear on RC. \nTo verify that a UMG is available on RC, run the following command on RC:\nAfter a UMG is created, the owner can submit a request toicds@psu.eduto associate this UMG with the<owner>_collabgroup. \nOnce the association between the UMG and the<owner>_collabgroup is made, then the group owner has full dynamic control over the access and roles of the<owner>_collabgroup by modifying the UMG membership. \nAfter this single request to ICDS, the owner no longer must submit requests to modify group membership and instead can manage the group directly. \nNote that any user added as a UMG member thatdoes nothave an active Roar account will not have access to Roar or any data on Roar until that user has an active Roar account.\nRC offers many file transfer options, and these methods are described in the following sections on this page.\nTo comply with restricted data storage standards, RR must adhere to a Secure Data Transfer Management Model. \nThe data transfer process is described in detail in theRoar Restricted Addendum.\nThe recommended file transfer method for RC isGlobus, especially for files that are multiple GBs in size or larger. \nAlso, if issues due to an unreliable connection arise, transferring via Globus may be a good option. \nGlobus is a web-based file transfer tool that automates the activity of managing file transfers, such as monitoring performance, retrying failed transfers, recovering from faults automatically whenever possible, and reporting status.\nWith Globus, users can easily, reliably, and securely transfer data to and from RC.\nGlobus endpoints must be installed on both the source and destination systems. \nRC has Globus endpoints available.\nTo transfer files with Globus, visit theGlobus websiteand log in as a Penn State user with your Penn State access account. \nSelect the File Manager tab on the left side of the Globus web interface and select the source and destination endpoints. \nThe endpoints may require an additional login. \nThe files and locations for the transfer can be selected graphically, and the transfer can be initiated by selectingStartabove the source endpoint file preview window. \nThe transfer will be handled by Globus, and typically, successful completion of the transfer will generate an email to your Penn State email.\nUsers can also download files from RC to their local device or upload files directly from their local device to RC using simple web interface operations. \nTo download a file, right-click the file and select Download. \nTo upload a file, select Upload from the Pane 1 Menu on the right.\nGlobus provides detailed instructions on the following topics:\nThe File Manager app on the RC Portal offers a very intuitive interface for file management. \nFiles can be moved, edited, uploaded, and downloaded with relative ease using this utility. \nUsers should limit the use of the RC Portal file manager utility to dealing with small files only.\nUsers may use thescpcommand to transfer files to and from RC. \nFor small-scale file transfers to/from RC, the submit nodes (hostnamesubmit.hpc.psu.edu) can be used. \nIt is typically practical to zip files together when transferring many files at once. \nIn a terminal session, thescpcommand can be used to transfer files in the following way:\nSome examples will further clarify the usage of this command. \nThe two locations in this example are the directory/home/abcon a local laptop device and the user scratch space on RC. \nIf a file namedlocal.filein/home/abcis to be transferred to a user's scratch space, the user should run the following command from a terminal session on the local laptop:\nAlternatively, if the user navigates to the/home/abclocation on their local laptop, the command can be slightly simplified to\nIf a directory nameddatadirlocated on the user's RC scratch space is to be transferred to the local laptop's/home/abcdirectory, the user can run the following from a terminal session on the local laptop:\nNote that since a directory is being transferred, the-roption must be used for thescpcommand so both the directory and its contents are transferred. \nIf the user navigates the terminal to the/home/abcdirectory to conduct the transfer, then the/home/abc/in the above commands can be replaced with a single period.to denote thecurrent working directory.\nThesftpcommand can also be used to transfer files and is more useful when transferring multiple smaller files in a piecemeal fashion. \nThis method allows for interactive navigation on the remote connection. \nFor small-scale file transfers to/from RC, the submit nodes (hostnamesubmit.hpc.psu.edu) can be used. \nFrom a local device, ansftpconnection can be made with\nTo spawn ansftpconnection on RC, use\nAfter thesftpconnection is made, navigational commands (i.e.ls,cd, etc) are performed on the remote connection normally, while navigational commands are performed on the local connection by appending the letterl(lowercase L) to the commands (i.e.lls,lcd, etc). \nFiles are transferred from the local device to the remote device using theput <filename>command, and files are transferred from the remote device to the local device using theget <filename>command. \nThe connection is terminated with theexitcommand.\nYet another file transfer option isrsync. \nThersynctool is widely used for backups and mirroring and as an improved copy command for everyday use. \nThersynccommand takes the form\nThersynctool should only be used within an interactive compute session due to its underlying resource requirements."
    },
    {
        "title": "Using Software",
        "url": "https://icds-docs.readthedocs.io/en/latest/05_UsingSoftware/",
        "content": "The software stack on Roar provides a wide variety of software to the entire user community. \nThere are two software stacks available.\nThe central software stack usesLmodto package the available software. \nLmod is a useful tool for managing user software environments using environment modules that can be dynamically added or removed using module files. \nLmod is hierarchical, so sub-modules can be nested under a module that is dependent upon. \nLmod alters environment variables, most notably the$PATHvariable, in order to make certain software packages reachable by the user environment.\nThe central software stack is available to the user environment by default since the$MODULEPATHenvironment variable is set and contains the central software stack location. \nModules can be directly loaded with\nTo see the available software modules, use\nAlthough a large variety of software packages are available via the system and central software stacks, users may need access to additional software. \nUsers may also wish to have greater control over the software packages that are required for their research workflow.\nUsers can install custom software packages and build custom software modules to build a custom user- or group-specific software stack. \nFor users and groups with well-defined research workflows, it is recommended to create a custom software stack to keep close control of the software installation versions and configuration. \nA location should be specified that contains the custom software installations and the module files for the custom software installations should be stored together in a common location. \nThis module location can be added to the$MODULEPATHenvironment variable so users can access the software modules just as they would for the central software stack. \nTheLmoddocumentation contains more detailed information on creating custom software modules.\nAnacondais a very useful package manager that is available on Roar. \nPackage managers simplify software package installation and manage dependency relationships while increasing both the repeatability and the portability of software. \nThe user environment is modified by the package manager so the shell can access different software packages. \nAnaconda was originally created for Python, but it can package and distribute software for any language. \nIt is usually very simple to create and manage new environments, install new packages, and import/export environments. \nMany packages are available for installation through Anaconda, and it enables retaining the environments in a silo to reduce cross-dependencies between different packages that may perturb environments.\nAnaconda can be loaded from the software stack on RC with the following command:\nUsage of Anaconda may cause storage quota issues since environments and packages are stored within~/.condaby default. \nThis issue can be easily resolved by moving the~/.condadirectory to the work directory and creating a link in its place pointing to the new location in the work directory. \nThis is described further in theHandling Datasection onManaging Large Configuration Files.\nAfter loading theanacondamodule, environments can be created and packages can be installed within those environments. \nWhen using theanacondamodule for the first time on a system, theconda init bashcommand may be required to initialize anaconda, then a new session must be started for the change to take effect. \nIn the new session, the command prompt may be prepended with(base)which denotes that the session is in the base anaconda environment.\nTo create an environment that contains bothnumpyandscipy, for example, run the following commands:\nNote that after the environment is entered, the leading item in the prompt changes to reflect the current environment.\nAlternatively, the creation of an environment and package installation can be completed with a single line.\nFor more detailed information on usage, check out theAnaconda documentation.\nSlurm does not automatically source the~/.bashrcfile in your batch job, so Anaconda may not be properly initialized within Slurm job submission scripts. \nFortunately, theanacondamodules on the software stack initialize the software so that thecondacommand is automatically available within the Slurm job submission script. \nIf using a different anaconda installation, this issue can be resolved by directly sourcing the~/.bashrcfile in your job script before running any conda commands:\nAlternatively, the environment can be activated usingsourceinstead ofconda.\nAnother way to resolve this is to add the following shebang to the top of a Slurm job script:\nYet another option would be to put the following commands before activating the conda environment:\nTo reiterate, theanacondamodules available on the software stack are configured such that thecondacommand is automatically available within a Slurm job submission script. \nThe above options are only necessary for other anaconda installations.\nEnvironments built with Anaconda can be used in Interactive Apps on the Roar Portals as well. \nTypically the environment should be created and configured in an interactive compute session, and then some additional steps are needed to make the environment available from within an Interactive App.\nTo access a conda environment within a Jupyter Server session, theipykernelpackage must be installed within the environment. \nTo do so, enter the environment and run the following commands:\nAfter theipykernelpackage is successfully installed within this environment, a Jupyter Server session can be launched via the Roar Portal. \nWhen submitting the form to launch the session, under theConda environment typefield, select theUse custom text fieldoption from the dropdown menu. \nThen enter the following into theEnvironment Setuptext field:\nAfter launching and entering the session, the environment is displayed in the kernel list.\nTo launch an RStudio Interactive App session, RStudio must have access to an installation of R. \nR can either be installed within the conda environment itself, or it can be loaded from the software stack. \nTypically, R will be installed by default when installing R packages within a conda environment; therefore, it is recommended when using conda environments within RStudio to simply utilize the environment's own R installation. \nTo create an environment containing an R installation, run the following command:\nAlternatively, R can simply be added to an existing environment by entering that environment and installing using the following command:\nR packages can installed directly via Anaconda within the environment as well. \nR packages available in Anaconda are usually namedr-<package name>such asr-plot3d,r-spatial, orr-ggplot.\nAfter R and any necessary R packages are installed within the environment, an RStudio session can be launched via the Roar Portal. \nWhen submitting the form to launch the session, under theEnvironment typefield, select theUse custom text fieldoption from the dropdown menu. \nThen enter the following into theEnvironment Setuptext field:\nPlease note that the default location of conda environments is in~/.conda/envs, which is why theCONDAENVLIBvariable is being set to~/.conda/envs/<environment>/lib. \nIf the environment is instead installed a non-default location, then theCONDAENVLIBvariable should be set accordingly. \nThe twoexportcommands in the block above are required because RStudio often has an issue loading some libraries while accessing the conda environment's R installation. \nExplicitly adding the conda environment'slibdirectory to theLD_LIBRARY_PATHvariable seems to clear up this issue.\nCompiling software from source is the most involved option for using software, but it gives the user the highest level of control. \nResearch computing software is often developed by academic researchers that do not place a large effort on packaging their software so that it can be easily deployed on other systems. \nIf the developer does not package the software using a package manager, then the only option is to build the software from source. \nIt is best to follow the installation instructions from the developer to successfully install the software from source.\nIt is recommended to build software on a node with the same processor type that will be used for running the software. \nOn a compute node, running the following command displays the processor type:\nSoftware builds are not typically back-compatible and will not run successfully on processors older than the processor used to build. \nIt is recommended to build on haswell (the oldest processor architecture on Roar) if you wish to have full compatibility across any Roar compute node. \nTo optimize for performance, however, build on the same processor on which the software runs.\nA container is a standard unit of software with two modes:\nA container is an abstraction at the application layer. \nMultiple containers can run on the same machine and share the host kernel with other containers, each running as isolated processes.\nApptainer is a secure container platform designed for HPC use cases and is available on Roar. \nContainers (or images) can either be pulled directly from a container repository or can be built from a definition file. \nA definition file or recipe file contains everything required to build a container. \nBuilding containers requires root privileges, so containers are built on your personal device and can be deployed on Roar. Alternatively, users can utilize the--fakerootoption to build containers without root privileges, and the usage of this method is described in Apptainer's documentation of thefakeroot feature.\nSoftware is continuously growing in complexity which can make managing the required user environment and wrangling dependent software an intractable problem. \nContainers address this issue by storing the software and all its dependencies (including a minimal operating system) in a single image file, eliminating the need to install additional packages or alter the runtime environment. \nThis makes the software both shareable and portable while the output becomes reproducible.\nIn a Slurm submission script, a container can be called serially using the following run line:\nTo use a container in parallel with MPI, the MPI library within the container must be compatible with the MPI implementation on the system, meaning that the MPI version on the system must generally be newer than the MPI version within the container. \nMore details on using MPI with containers can be found on Apptainer'sApptainer and MPI Applicationspage. \nIn a Slurm submission script, a container with MPI can be called using\nContainers change the user space into a swappable component, and provide the following benefits:\nContainer images can be made publicly available, and containers for many use cases can be found at the following container registries:\nContainers can be made from scratch using adefinition file, or recipe file, which is a text file that specifies the base image, the software to be installed, and other information. \nTheapptainer buildcommand'sdocumentationshows the full usage for the build command. \nContainer images can also be bootstrapped from other images, found on Docker Hub for instance.\nThe recommended workflow for building containers is shown below:\n\nPython is a high-level, general-purpose programming language.\nPython is available by default to all users on the system software stack, and it is also available on the central software stack. \nAdditionally, users can install their own instances of Python in a variety of ways in either their userspace or in group spaces.\nPython packages can be installed easily usingpip. \nBy default,pipwill attempt to install packages to a system location.\nOn shared systems, however, users do not have write access to system locations. \nThe packages can instead be installed in~/.local, which is a user location, using the following:\nAlso, packages can be installed to a custom specified location using the--targetoption:\nNote that ifpipis not available, simply trypip3(for python3) orpip2(for python2) instead.\nR is a free software environment for statistical computing and graphics.\nR users should make sure that the version of R remains consistent. \nSeveral R versions are available, and when a package is installed in one version, it is not always accessible when operating in another version. \nAlways check the R version and remain consistent! \nR modules can be loaded from the central software stack, and R can also be installed by users in a variety of ways within their userspace or group spaces.\nR manages some dependencies and versions through the CRAN-like repos. \nR packages can be installed from within the R console with the following command:\nUpon running the install command, a warning usually appears stating that the default system install location is not writable, so it asks to install in a personal library instead. \nAfter entering \"yes\" as a response, it may then ask to create a personal library location. \nResponding \"yes\" again will proceed with the installation, probably by asking to select a CRAN repository.\nThe default personal directory described above will install the package within the~/R/directory. \nAn install location can instead be supplied to the install command using thelibargument:\nAfter installation, packages can then be loaded using the following command in the R console:\nIf the package was installed in a non-standard location, then the package can be loaded from that custom install location using thelic.locargument of thelibrary()command:\nAnother method to specify package installation locations for R is to modify theR_LIBSenvironment variable before launching an R console session. \nIf RStudio is being used, though, theR_LIBS_USERenvironment variable must be modified before launching RStudio. \nModifying these environment variables properly can eliminate the need to use thelib.locoption of R'slibrary()command.\nIt is recommended to review dependencies of any packages to be installed because additional software may have to be loaded in the environment before launching the R console. \nFor example, some R packages utilize CMake to perform the installation. \nIn that case, thecmakemodule should be loaded before launching the R console session.\nTo install the ggplot2 R package, first search ggplot2 online to see if there are installation instructions. \nA quick search shows that ggplot2 is included in the tidyverse package and that the recommended installation instructions are the following:\nSearching for install instructions usually provides all the necessary information!\nSome R packages may require changes to the user environment before the package can be installed successfully within the R console. \nTypically, the user environment change is as simple as accessing a newer compiler version by loading a software module likeintelwith\nSometimes, installing R packages may be a little more involved. \nTo install theunitsR package, for example, an additional library must be downloaded and installed locally for the package to be installed properly. \nTo install theunitsR package for R version 4.2.1, perform the following commands in an interactive session on a compute node:\nThe R installation itself and its R packages can be easily installed and managed within a conda environment. \nCreating a conda environment containing its own R installation and some R packages can be accomplished with the following:\nNote that afterr-baseis the base R installation, and R packages (or in the case oftidyverse, a bundle of packages) usually are namedr-*in conda repos.\nAlternatively, the creation of this environment can be completed with a single line."
    },
    {
        "title": "Roar Restricted Addendum",
        "url": "https://icds-docs.readthedocs.io/en/latest/06_RoarRestricted/",
        "content": "Roar Restricted (RR) is configured for the handling of restricted data and is dedicated to serving the small portion of researchers who must comply with more stringent data storage standards.\nRestricted group storage on RR is provided on an as-needed basis to a principal investigator (PI) specifically handling restricted data.\nMost of the material within this ICDS User Guide is common to both Roar Collab (RC) and RR, but some sections specifically refer to RC.\nThisRoar Restricted Addendumspecifically addresses items unique to RR.\nRR system specificationsandRR storage locationsare described within the linked Roar User Guide sections.\nRR accounts are only granted to individuals that require access to an active restricted storage allocation. \nAlso, RR does offer a free compute account, so users must submit jobs to a compute account provided by a paid compute allocation.\nTo request an account on RR, a user must complete theAccount Requestform and then send an email toicds@psu.eduwith the user's PSU access account ID, the restricted storage owner's name and access account ID, and an indication that they are requesting an account on RR.\nNon-faculty accounts require approval from a faculty/PI sponsor. \nThe user will then be assigned an RR training module.\nTo gain and retain access to RR, users must complete the RR training module viaPenn State LRNwhich is required by the Office of Information Security (OIS) to maintain compliance with the Authority to Operate (ATO).\nThe account creation process takes about 48 hours after faculty/PI sponsor approval.\nUsers can only connect to RR via theRR Portal(rrportal.hpc.psu.edu).\nRR is only accessible when connecting either via the Penn State network or via Penn State GlobalProtect VPN. \nGlobalProtect can be downloaded fromit.psu.edu/softwareandadditional configuration instructionsare available from Penn State IT.\nRR is designed to limit the risk of accidental data leaks, so ICDS is implementing a data transfer process that relies on a PI-appointed data administrator to transfer data to/from RR.\nPIs and any appointed data administrators are responsible for the data and any transfers conducted via the data manager node.\nPIs can request access to the file transfer capability via the Data Transfer Access Form. \nPIs and any data administrators must attest annually that they still need access to the data manager node.\nTo request data administrator access, a user must emailicds@psu.eduwith the faculty/PI sponsor copied. The faculty/PI sponsor must approve that the user should be granted data administrator status. \nUpon faculty/PI confirmation, the user must submit theData Transfer System Access Request Form. \nThe user will then be added to the necessary groups on RR to grant them access to the data manager nodes.\nTo comply with restricted data storage standards, Roar Restricted (RR) must adhere to a Secure Data Transfer Management Model. \nUsers cannot transfer data on or off RR, but they can move data between the main restricted group storage and the group's staging area. \nData administrators may transfer data on and off RR via the data manager node using a group's staging area. \nWhile conducting transfers, data administrators are responsible for adhering to the data management standards and guidelines.\nTransfers are conducted using the process outlined in the diagrams below.\n"
    }
]